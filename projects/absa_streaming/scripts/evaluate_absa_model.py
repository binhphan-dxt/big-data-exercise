# SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# evaluate_absa_model.py
# ======================================
# Script ƒë√°nh gi√° m√¥ h√¨nh ABSA m·ªõi v√† so s√°nh v·ªõi m√¥ h√¨nh hi·ªán t·∫°i
# Tr·∫£ v·ªÅ metrics v√† quy·∫øt ƒë·ªãnh c√≥ n√™n deploy hay kh√¥ng

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
import os
import sys
import json
import glob
from datetime import datetime
import gc
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# TƒÉng threads v·ªõi 16GB Docker RAM
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"
torch.set_num_threads(4)

# T·∫Øt caching c·ªßa tokenizer ƒë·ªÉ ti·∫øt ki·ªám RAM
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# === C·∫•u h√¨nh ===
ASPECTS = ["Price", "Shipping", "Outlook", "Quality", "Size", "Shop_Service", "General", "Others"]
# D√πng distilbert-base-multilingual-cased ƒë·ªÉ match v·ªõi train script v√† ti·∫øt ki·ªám RAM
MODEL_NAME = "distilbert-base-multilingual-cased"
MAX_LEN = 64  # TƒÉng l√™n 64 v·ªõi 16GB RAM ƒë·ªÉ match train script
DEVICE = "cpu"  # Force CPU ƒë·ªÉ tr√°nh CUDA overhead tr√™n Mac M chip
BATCH_SIZE = 8  # TƒÉng l√™n 8 v·ªõi 16GB RAM
MAX_EVAL_SAMPLES = None  # Kh√¥ng gi·ªõi h·∫°n - d√πng t·∫•t c·∫£ d·ªØ li·ªáu test

# ƒê∆∞·ªùng d·∫´n
DATA_PATH = "/opt/airflow/projects/absa_streaming/data/test_data.csv"
MODELS_DIR = "/opt/airflow/models"
CURRENT_MODEL_PATH = "/opt/airflow/models/best_absa_hardshare.pt"
TRAINED_MODEL_PREFIX = "absa_model_retrained"
EVALUATION_RESULTS_DIR = "/opt/airflow/models/evaluation_results"

# === ƒê·ªãnh nghƒ©a m√¥ h√¨nh ABSA (gi·ªëng train script) ===
class ABSAModel(nn.Module):
    def __init__(self, model_name=MODEL_NAME, num_aspects=len(ASPECTS)):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        H = self.backbone.config.hidden_size
        self.dropout = nn.Dropout(0.1)
        self.head_m = nn.Linear(H, num_aspects)
        self.head_s = nn.Linear(H, num_aspects * 3)
    
    def forward(self, input_ids, attention_mask):
        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = self.dropout(out.last_hidden_state[:, 0, :])
        return self.head_m(h_cls), self.head_s(h_cls).view(-1, len(ASPECTS), 3)

# === Dataset ===
class ABSADataset(Dataset):
    def __init__(self, texts, aspect_labels, sentiment_labels, tokenizer, max_len=MAX_LEN):
        self.texts = texts
        self.aspect_labels = aspect_labels
        self.sentiment_labels = sentiment_labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "aspect_labels": torch.tensor(self.aspect_labels[idx], dtype=torch.float),
            "sentiment_labels": torch.tensor(self.sentiment_labels[idx], dtype=torch.long)
        }

# === H√†m load v√† preprocess d·ªØ li·ªáu ===
def load_and_preprocess_data(data_path):
    """Load v√† preprocess d·ªØ li·ªáu t·ª´ CSV"""
    print(f"[Evaluate] ƒêang load d·ªØ li·ªáu t·ª´ {data_path}...")
    df = pd.read_csv(data_path)
    
    df = df[df["Review"].notna()]
    texts = df["Review"].tolist()
    
    aspect_labels = []
    sentiment_labels = []
    
    for _, row in df.iterrows():
        aspect_row = []
        sentiment_row = []
        for asp in ASPECTS:
            val = row[asp]
            if pd.isna(val) or val == -1:
                aspect_row.append(0)
                sentiment_row.append(0)
            else:
                aspect_row.append(1)
                if val == 1:
                    sentiment_row.append(1)  # POS
                elif val == 2:
                    sentiment_row.append(2)  # NEG
                else:
                    sentiment_row.append(0)  # NEU
        
        aspect_labels.append(aspect_row)
        sentiment_labels.append(sentiment_row)
    
    print(f"[Evaluate] ƒê√£ load {len(texts)} m·∫´u d·ªØ li·ªáu.")
    return texts, aspect_labels, sentiment_labels

# === H√†m ƒë√°nh gi√° m√¥ h√¨nh ===
def evaluate_model(model, data_loader, device):
    """ƒê√°nh gi√° m√¥ h√¨nh v√† tr·∫£ v·ªÅ metrics"""
    model.eval()
    
    total_aspect_correct = 0
    total_aspect_predicted = 0
    total_aspect_actual = 0
    
    total_sentiment_correct = 0
    total_sentiment_predicted = 0
    
    total_loss = 0.0
    aspect_criterion = nn.BCEWithLogitsLoss()
    sentiment_criterion = nn.CrossEntropyLoss(ignore_index=-1)
    
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            aspect_labels = batch["aspect_labels"].to(device)
            sentiment_labels = batch["sentiment_labels"].to(device)
            
            logits_m, logits_s = model(input_ids, attention_mask)
            
            # Loss
            loss_aspect = aspect_criterion(logits_m, aspect_labels)
            loss_sentiment = 0.0
            for i in range(len(ASPECTS)):
                mask = aspect_labels[:, i] == 1
                if mask.sum() > 0:
                    loss_sentiment += sentiment_criterion(
                        logits_s[mask, i, :],
                        sentiment_labels[mask, i]
                    )
            loss_sentiment = loss_sentiment / len(ASPECTS)
            total_loss += (loss_aspect + loss_sentiment).item()
            
            # Aspect detection metrics
            aspect_preds = (torch.sigmoid(logits_m) > 0.5).float()
            total_aspect_correct += (aspect_preds == aspect_labels).sum().item()
            total_aspect_predicted += aspect_preds.sum().item()
            total_aspect_actual += aspect_labels.sum().item()
            
            # Sentiment classification metrics (ch·ªâ t√≠nh cho c√°c aspect c√≥ trong label)
            sentiment_preds = torch.argmax(logits_s, dim=2)
            for i in range(len(ASPECTS)):
                mask = aspect_labels[:, i] == 1
                if mask.sum() > 0:
                    total_sentiment_correct += (sentiment_preds[mask, i] == sentiment_labels[mask, i]).sum().item()
                    total_sentiment_predicted += mask.sum().item()
            
            # Gi·∫£i ph√≥ng memory m·ªói batch
            del input_ids, attention_mask, aspect_labels, sentiment_labels
            del logits_m, logits_s, loss_aspect, loss_sentiment
            del aspect_preds, sentiment_preds
            gc.collect()
    
    avg_loss = total_loss / len(data_loader)
    
    # T√≠nh metrics
    aspect_precision = total_aspect_correct / total_aspect_predicted if total_aspect_predicted > 0 else 0
    aspect_recall = total_aspect_correct / total_aspect_actual if total_aspect_actual > 0 else 0
    aspect_f1 = 2 * aspect_precision * aspect_recall / (aspect_precision + aspect_recall) if (aspect_precision + aspect_recall) > 0 else 0
    
    sentiment_accuracy = total_sentiment_correct / total_sentiment_predicted if total_sentiment_predicted > 0 else 0
    
    metrics = {
        "loss": avg_loss,
        "aspect_precision": aspect_precision,
        "aspect_recall": aspect_recall,
        "aspect_f1": aspect_f1,
        "sentiment_accuracy": sentiment_accuracy,
        "overall_score": (aspect_f1 + sentiment_accuracy) / 2  # Combined score
    }
    
    return metrics

# === H√†m t√¨m m√¥ h√¨nh m·ªõi nh·∫•t ===
def find_latest_retrained_model():
    """T√¨m m√¥ h√¨nh retrained m·ªõi nh·∫•t"""
    pattern = os.path.join(MODELS_DIR, f"{TRAINED_MODEL_PREFIX}_*.pt")
    model_files = glob.glob(pattern)
    
    if not model_files:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh retrained trong {MODELS_DIR}")
    
    # S·∫Øp x·∫øp theo th·ªùi gian t·∫°o (m·ªõi nh·∫•t tr∆∞·ªõc)
    model_files.sort(key=os.path.getmtime, reverse=True)
    latest_model = model_files[0]
    
    print(f"[Evaluate] T√¨m th·∫•y m√¥ h√¨nh m·ªõi nh·∫•t: {latest_model}")
    return latest_model

# === H√†m ƒë√°nh gi√° v√† so s√°nh ===
def evaluate_and_compare():
    """ƒê√°nh gi√° m√¥ h√¨nh m·ªõi v√† so s√°nh v·ªõi m√¥ h√¨nh hi·ªán t·∫°i"""
    sys.stdout.reconfigure(encoding='utf-8')
    
    print("=" * 60)
    print("üìä B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh ABSA")
    print("=" * 60)
    
    # Load d·ªØ li·ªáu test
    texts, aspect_labels, sentiment_labels = load_and_preprocess_data(DATA_PATH)
    
    # Chia test set (20% cu·ªëi)
    split_idx = int(0.8 * len(texts))
    test_texts = texts[split_idx:]
    test_aspects = aspect_labels[split_idx:]
    test_sentiments = sentiment_labels[split_idx:]
    
    print(f"[Evaluate] Test set: {len(test_texts)} m·∫´u")
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
    
    # Test dataset
    test_dataset = ABSADataset(test_texts, test_aspects, test_sentiments, tokenizer)
    # Gi·∫£m num_workers ƒë·ªÉ ti·∫øt ki·ªám RAM
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)
    
    # T√¨m m√¥ h√¨nh m·ªõi nh·∫•t
    new_model_path = find_latest_retrained_model()
    
    # Load m√¥ h√¨nh m·ªõi
    print(f"[Evaluate] ƒêang load m√¥ h√¨nh m·ªõi: {new_model_path}")
    # Gi·∫£i ph√≥ng memory tr∆∞·ªõc khi load model
    gc.collect()
    
    new_model = ABSAModel()
    new_model.load_state_dict(torch.load(new_model_path, map_location=DEVICE))
    new_model.to(DEVICE)
    
    # Freeze backbone ƒë·ªÉ ti·∫øt ki·ªám RAM trong evaluation
    for param in new_model.backbone.parameters():
        param.requires_grad = False
    
    # ƒê√°nh gi√° m√¥ h√¨nh m·ªõi
    print("\n[Evaluate] ƒêang ƒë√°nh gi√° m√¥ h√¨nh m·ªõi...")
    new_metrics = evaluate_model(new_model, test_loader, DEVICE)
    
    # Gi·∫£i ph√≥ng memory sau khi ƒë√°nh gi√°
    del new_model
    gc.collect()
    
    print(f"\n[Evaluate] üìä K·∫øt qu·∫£ m√¥ h√¨nh m·ªõi:")
    print(f"  Loss: {new_metrics['loss']:.4f}")
    print(f"  Aspect F1: {new_metrics['aspect_f1']:.4f}")
    print(f"  Sentiment Accuracy: {new_metrics['sentiment_accuracy']:.4f}")
    print(f"  Overall Score: {new_metrics['overall_score']:.4f}")
    
    # ƒê√°nh gi√° m√¥ h√¨nh hi·ªán t·∫°i (n·∫øu c√≥ v√† t∆∞∆°ng th√≠ch)
    current_metrics = None
    if os.path.exists(CURRENT_MODEL_PATH):
        print(f"\n[Evaluate] ƒêang ƒë√°nh gi√° m√¥ h√¨nh hi·ªán t·∫°i: {CURRENT_MODEL_PATH}")
        try:
            # Gi·∫£i ph√≥ng memory tr∆∞·ªõc khi load model
            gc.collect()
            
            current_model = ABSAModel()
            state_dict = torch.load(CURRENT_MODEL_PATH, map_location=DEVICE)
            
            # Th·ª≠ load v·ªõi strict=False ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng t∆∞∆°ng th√≠ch
            try:
                current_model.load_state_dict(state_dict, strict=True)
                print(f"[Evaluate] ‚úÖ ƒê√£ load weights th√†nh c√¥ng (strict mode)")
            except RuntimeError as e:
                # N·∫øu kh√¥ng t∆∞∆°ng th√≠ch, th·ª≠ load v·ªõi strict=False
                print(f"[Evaluate] ‚ö†Ô∏è Model kh√¥ng t∆∞∆°ng th√≠ch (c√≥ th·ªÉ train v·ªõi model kh√°c), th·ª≠ load v·ªõi strict=False...")
                try:
                    current_model.load_state_dict(state_dict, strict=False)
                    print(f"[Evaluate] ‚ö†Ô∏è ƒê√£ load weights v·ªõi strict=False (m·ªôt s·ªë weights kh√¥ng match)")
                except Exception as e2:
                    print(f"[Evaluate] ‚ùå Kh√¥ng th·ªÉ load weights: {e2}")
                    print(f"[Evaluate] M√¥ h√¨nh hi·ªán t·∫°i kh√¥ng t∆∞∆°ng th√≠ch v·ªõi architecture hi·ªán t·∫°i (distilbert vs xlm-roberta)")
                    print(f"[Evaluate] S·∫Ω b·ªè qua vi·ªác so s√°nh v√† deploy m√¥ h√¨nh m·ªõi n·∫øu t·ªët h∆°n baseline")
                    current_model = None
            
            if current_model is not None:
                current_model.to(DEVICE)
                # Freeze backbone ƒë·ªÉ ti·∫øt ki·ªám RAM
                for param in current_model.backbone.parameters():
                    param.requires_grad = False
                
                current_metrics = evaluate_model(current_model, test_loader, DEVICE)
                
                print(f"\n[Evaluate] üìä K·∫øt qu·∫£ m√¥ h√¨nh hi·ªán t·∫°i:")
                print(f"  Loss: {current_metrics['loss']:.4f}")
                print(f"  Aspect F1: {current_metrics['aspect_f1']:.4f}")
                print(f"  Sentiment Accuracy: {current_metrics['sentiment_accuracy']:.4f}")
                print(f"  Overall Score: {current_metrics['overall_score']:.4f}")
                
                # Gi·∫£i ph√≥ng memory
                del current_model
                gc.collect()
        except Exception as e:
            print(f"[Evaluate] ‚ùå L·ªói khi ƒë√°nh gi√° m√¥ h√¨nh hi·ªán t·∫°i: {e}")
            print(f"[Evaluate] M√¥ h√¨nh hi·ªán t·∫°i kh√¥ng t∆∞∆°ng th√≠ch ho·∫∑c b·ªã l·ªói")
            print(f"[Evaluate] S·∫Ω b·ªè qua vi·ªác so s√°nh v√† deploy m√¥ h√¨nh m·ªõi n·∫øu t·ªët h∆°n baseline")
            current_metrics = None
    else:
        print(f"\n[Evaluate] ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh hi·ªán t·∫°i: {CURRENT_MODEL_PATH}")
        print(f"[Evaluate] M√¥ h√¨nh m·ªõi s·∫Ω ƒë∆∞·ª£c deploy.")
    
    # So s√°nh v√† quy·∫øt ƒë·ªãnh
    should_deploy = False
    improvement = None
    if current_metrics is None:
        should_deploy = True
        reason = "Kh√¥ng c√≥ m√¥ h√¨nh hi·ªán t·∫°i"
    else:
        # So s√°nh overall_score (F1 + Accuracy) / 2
        improvement = new_metrics['overall_score'] - current_metrics['overall_score']
        if improvement > 0.01:  # C·∫£i thi·ªán √≠t nh·∫•t 1%
            should_deploy = True
            reason = f"M√¥ h√¨nh m·ªõi t·ªët h∆°n ({improvement:.4f} ƒëi·ªÉm)"
        else:
            should_deploy = False
            reason = f"M√¥ h√¨nh m·ªõi kh√¥ng t·ªët h∆°n (ch√™nh l·ªách: {improvement:.4f})"
    
    # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°
    os.makedirs(EVALUATION_RESULTS_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    evaluation_result = {
        "timestamp": timestamp,
        "new_model_path": new_model_path,
        "current_model_path": CURRENT_MODEL_PATH if os.path.exists(CURRENT_MODEL_PATH) else None,
        "new_metrics": new_metrics,
        "current_metrics": current_metrics,
        "should_deploy": should_deploy,
        "reason": reason,
        "improvement": improvement if current_metrics else None
    }
    
    result_path = os.path.join(EVALUATION_RESULTS_DIR, f"evaluation_{timestamp}.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
    
    print(f"\n[Evaluate] ‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°: {result_path}")
    print(f"\n[Evaluate] üéØ Quy·∫øt ƒë·ªãnh: {'‚úÖ DEPLOY' if should_deploy else '‚ùå KH√îNG DEPLOY'}")
    print(f"[Evaluate] L√Ω do: {reason}")
    
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£
    return {
        "should_deploy": should_deploy,
        "new_model_path": new_model_path,
        "new_metrics": new_metrics,
        "current_metrics": current_metrics,
        "evaluation_result_path": result_path
    }

if __name__ == "__main__":
    try:
        result = evaluate_and_compare()
        if result["should_deploy"]:
            print(f"\n‚úÖ M√¥ h√¨nh m·ªõi ƒë·∫°t y√™u c·∫ßu v√† s·∫Ω ƒë∆∞·ª£c deploy!")
            sys.exit(0)
        else:
            print(f"\n‚ö†Ô∏è M√¥ h√¨nh m·ªõi kh√¥ng ƒë·∫°t y√™u c·∫ßu, kh√¥ng deploy.")
            sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi ƒë√°nh gi√°: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

