# # SE363 ‚Äì Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n n·ªÅn t·∫£ng d·ªØ li·ªáu l·ªõn
# # Khoa C√¥ng ngh·ªá Ph·∫ßn m·ªÅm ‚Äì Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, ƒêHQG-HCM
# # HopDT ‚Äì Faculty of Software Engineering, University of Information Technology (FSE-UIT)

# # consumer_postgres_streaming.py
# # ======================================
# # Consumer ƒë·ªçc d·ªØ li·ªáu t·ª´ Kafka topic "absa-reviews"
# # ‚Üí ch·∫°y inference m√¥ h√¨nh ABSA (.pt)
# # ‚Üí ghi k·∫øt qu·∫£ v√†o PostgreSQL
# # ‚Üí Airflow s·∫Ω gi√°m s√°t v√† kh·ªüi ƒë·ªông l·∫°i khi job b·ªã d·ª´ng.

# from pyspark.sql import SparkSession, functions as F, types as T
# from pyspark.sql.functions import pandas_udf, from_json, col
# import pandas as pd, torch, torch.nn as nn, torch.nn.functional as tF
# from transformers import AutoTokenizer, AutoModel
# import random, time, os, sys, json
# import warnings
# warnings.filterwarnings("ignore", category=FutureWarning)
# # === 1. Spark session v·ªõi Kafka connector ===
# scala_version = "2.12"
# spark_version = "3.5.1"

# spark = (
#     SparkSession.builder
#     .appName("Kafka_ABSA_Postgres")
#     .config("spark.jars.packages",
#             f"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},"
#             "org.postgresql:postgresql:42.6.0,"
#             "org.apache.kafka:kafka-clients:3.5.1")
#     .config("spark.executor.instances", "1")
#     .config("spark.executor.cores", "1")
#     .config("spark.driver.maxResultSize", "4g")
#     .config("spark.sql.streaming.checkpointLocation", "/opt/airflow/checkpoints/absa_streaming_checkpoint")
#     .config("spark.sql.execution.arrow.pyspark.enabled", "false")
#     .getOrCreate()
# )
# spark.sparkContext.setLogLevel("WARN")

# # === 2. ƒê·ªçc d·ªØ li·ªáu streaming t·ª´ Kafka ===
# df_stream = (
#     spark.readStream
#     .format("kafka")
#     .option("kafka.bootstrap.servers", "kafka:9092")
#     .option("subscribe", "absa-reviews")
#     .option("startingOffsets", "earliest")
#     .option("maxOffsetsPerTrigger", 5)
#     .load()
# )

# df_text = df_stream.selectExpr("CAST(value AS STRING) as Review")

# # === 3. ƒê·ªãnh nghƒ©a m√¥ h√¨nh ABSA ===
# ASPECTS = ["Price","Shipping","Outlook","Quality","Size","Shop_Service","General","Others"]
# MODEL_NAME = "xlm-roberta-base"
# MODEL_PATH = "/opt/airflow/models/best_absa_hardshare.pt"
# MAX_LEN = 64
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# _model, _tokenizer = None, None

# class ABSAModel(nn.Module):
#     def __init__(self, model_name=MODEL_NAME, num_aspects=len(ASPECTS)):
#         super().__init__()
#         self.backbone = AutoModel.from_pretrained(model_name)
#         H = self.backbone.config.hidden_size
#         self.dropout = nn.Dropout(0.1)
#         self.head_m = nn.Linear(H, num_aspects)
#         self.head_s = nn.Linear(H, num_aspects * 3)
#     def forward(self, input_ids, attention_mask):
#         out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
#         h_cls = self.dropout(out.last_hidden_state[:, 0, :])
#         return self.head_m(h_cls), self.head_s(h_cls).view(-1, len(ASPECTS), 3)

# @pandas_udf(T.ArrayType(T.FloatType()))
# def absa_infer_udf(texts: pd.Series) -> pd.Series:
#     global _model, _tokenizer
#     if _model is None:
#         _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
#         _model = ABSAModel()
#         _model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
#         _model.to(DEVICE).eval()

#     results = []
#     for t in texts:
#         enc = _tokenizer(t, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt").to(DEVICE)
#         with torch.no_grad():
#             logits_m, logits_s = _model(enc["input_ids"], enc["attention_mask"])
#             p_m = torch.sigmoid(logits_m)[0].cpu().numpy().tolist()
#             p_s = tF.softmax(logits_s, dim=-1)[0].cpu().numpy().flatten().tolist()
#         results.append(p_m + p_s)
#     return pd.Series(results)

# df_pred = df_text.withColumn("predictions", absa_infer_udf(F.col("Review")))

# # === 4. Gi·∫£i m√£ k·∫øt qu·∫£ ra nh√£n POS/NEG/NEU ===
# @pandas_udf("string")
# def decode_sentiment(preds: pd.Series) -> pd.Series:
#     SENTIMENTS = ["POS", "NEU", "NEG"]
#     res = []
#     for p in preds:
#         if not p:
#             res.append("?")
#             continue
#         p = list(p)
#         p_m, p_s = p[:len(ASPECTS)], p[len(ASPECTS):]
#         decoded = []
#         for i, asp in enumerate(ASPECTS):
#             triplet = p_s[i*3:(i+1)*3]
#             s = SENTIMENTS[int(max(range(3), key=lambda j: triplet[j]))]
#             decoded.append(f"{asp}:{s}")
#         res.append(", ".join(decoded))
#     return pd.Series(res)

# df_final = df_pred.withColumn("decoded", decode_sentiment(F.col("predictions")))
# for asp in ASPECTS:
#     df_final = df_final.withColumn(asp, F.regexp_extract("decoded", f"{asp}:(\\w+)", 1))

# # === Gi·∫£i m√£ Review JSON th√†nh text ti·∫øng Vi·ªát tr∆∞·ªõc khi stream ===
# review_schema = T.StructType([
#     T.StructField("id", T.StringType()),
#     T.StructField("review", T.StringType())
# ])
# df_final = df_final.withColumn("ReviewText", from_json(col("Review"), review_schema).getField("review"))

# # === 5. Ghi k·∫øt qu·∫£ v√†o PostgreSQL (chu·∫©n UTF-8, log ƒë·∫ßy ƒë·ªß, x·ª≠ l√Ω l·ªói an to√†n) ===
# def write_to_postgres(batch_df, batch_id):
#     sys.stdout.reconfigure(encoding='utf-8')
#     total_rows = batch_df.count()

#     if total_rows == 0:
#         print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
#         return

#     preview = batch_df.select("ReviewText", *ASPECTS).limit(5).toPandas().to_dict(orient="records")
#     print(f"\n[Batch {batch_id}] Nh·∫≠n {total_rows} d√≤ng, hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu:")
#     print(json.dumps(preview, ensure_ascii=False, indent=2))

#     # Gi·∫£ l·∫≠p l·ªói m√¥ ph·ªèng ƒë·ªÉ test Airflow restart
#     if batch_id % 5 == 0:
#         print(f"[Batch {batch_id}] üí• Gi·∫£ l·∫≠p s·ª± c·ªë: crash m√¥ ph·ªèng.")
#         raise Exception(f"Simulated crash at batch {batch_id}")

#     try:
#         (batch_df
#             .select("ReviewText", *ASPECTS)
#             .write
#             .format("jdbc")
#             .option("url", "jdbc:postgresql://postgres:5432/airflow")
#             .option("dbtable", "absa_results")
#             .option("user", "airflow")
#             .option("password", "airflow")
#             .option("driver", "org.postgresql.Driver")
#             .option("charset", "utf8")
#             .mode("append")
#             .save()
#         )
#         print(f"[Batch {batch_id}] ‚úÖ Ghi PostgreSQL th√†nh c√¥ng ({total_rows} d√≤ng).")
#         subset = batch_df.select("ReviewText", *ASPECTS).limit(3).toPandas().to_dict(orient="records")
#         print(f"[Batch {batch_id}] D·ªØ li·ªáu ƒë√£ ghi (m·∫´u):")
#         print(json.dumps(subset, ensure_ascii=False, indent=2))

#     except Exception as e:
#         print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng th·ªÉ ghi v√†o PostgreSQL, ghi log ra console thay th·∫ø.")
#         print(f"L·ªói: {str(e)}")
#         subset = batch_df.select("ReviewText", *ASPECTS).limit(5).toPandas().to_dict(orient="records")
#         print(json.dumps(subset, ensure_ascii=False, indent=2))

# # === 6. B·∫Øt ƒë·∫ßu stream ===
# query = (
#     df_final.writeStream
#     .foreachBatch(write_to_postgres)
#     .outputMode("append")
#     .trigger(processingTime="5 seconds")
#     .start()
# )

# print("üöÄ Streaming job started ‚Äî ƒëang l·∫Øng nghe d·ªØ li·ªáu t·ª´ Kafka...")
# query.awaitTermination()

# consumer_postgres_streaming.py
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.functions import pandas_udf, from_json, col
import pandas as pd, json, sys, torch, torch.nn as nn, torch.nn.functional as tF
from transformers import AutoTokenizer, AutoModel
import os
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# === 1. Spark session v·ªõi Kafka connector ===
scala_version = "2.12"
spark_version = "3.5.1"

spark = (
    SparkSession.builder
    .appName("Kafka_ABSA_Postgres")
    .config("spark.jars.packages",
            f"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version},"
            "org.postgresql:postgresql:42.6.0,"
            "org.apache.kafka:kafka-clients:3.5.1")
    .config("spark.executor.instances", "1")
    .config("spark.executor.cores", "1")
    .config("spark.driver.maxResultSize", "2g")  # Gi·∫£m t·ª´ 4g xu·ªëng 2g ƒë·ªÉ ti·∫øt ki·ªám RAM
    .config("spark.sql.streaming.checkpointLocation", "/opt/airflow/checkpoints/absa_streaming_checkpoint")
    .config("spark.sql.execution.arrow.pyspark.enabled", "false")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# === 2. ƒê·ªçc d·ªØ li·ªáu streaming t·ª´ Kafka ===
df_stream = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:9092")
    .option("subscribe", "absa-reviews")
    .option("startingOffsets", "earliest")
    .option("maxOffsetsPerTrigger", 5)  # batch nh·ªè
    .load()
)

df_text = df_stream.selectExpr("CAST(value AS STRING) as Review")

# === 3. ƒê·ªãnh nghƒ©a m√¥ h√¨nh ABSA (d√πng m√¥ h√¨nh nh·∫π) ===
ASPECTS = ["Price","Shipping","Outlook","Quality","Size","Shop_Service","General","Others"]
# D√πng distilbert-base-multilingual-cased thay v√¨ xlm-roberta-base ƒë·ªÉ ti·∫øt ki·ªám RAM (~500MB vs ~1GB)
MODEL_NAME = "distilbert-base-multilingual-cased"
MODEL_PATH = "/opt/airflow/models/best_absa_hardshare.pt"
MAX_LEN = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Global variables ƒë·ªÉ cache model v√† tokenizer
_model, _tokenizer = None, None
_model_load_failed = False  # Flag ƒë·ªÉ ƒë√°nh d·∫•u ƒë√£ th·ª≠ load v√† fail

class ABSAModel(nn.Module):
    def __init__(self, model_name=MODEL_NAME, num_aspects=len(ASPECTS)):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        H = self.backbone.config.hidden_size
        self.dropout = nn.Dropout(0.1)
        self.head_m = nn.Linear(H, num_aspects)  # Multi-label classification cho aspects
        self.head_s = nn.Linear(H, num_aspects * 3)  # Sentiment classification (POS/NEU/NEG)
    
    def forward(self, input_ids, attention_mask):
        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = self.dropout(out.last_hidden_state[:, 0, :])
        return self.head_m(h_cls), self.head_s(h_cls).view(-1, len(ASPECTS), 3)

@pandas_udf(T.ArrayType(T.FloatType()))
def absa_infer_udf(texts: pd.Series) -> pd.Series:
    global _model, _tokenizer, _model_load_failed
    
    # N·∫øu ƒë√£ th·ª≠ load v√† fail, d√πng fallback lu√¥n
    if _model_load_failed:
        return _fallback_prediction(texts)
    
    # Load model v√† tokenizer m·ªôt l·∫ßn (lazy loading)
    if _model is None:
        print("[Model] ƒêang load m√¥ h√¨nh ABSA...")
        try:
            _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
            _model = ABSAModel()
            
            # Th·ª≠ load t·ª´ file, n·∫øu kh√¥ng c√≥ ho·∫∑c kh√¥ng t∆∞∆°ng th√≠ch th√¨ d√πng pretrained
            if os.path.exists(MODEL_PATH):
                try:
                    print(f"[Model] ƒêang load weights t·ª´ {MODEL_PATH}...")
                    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
                    _model.load_state_dict(state_dict, strict=False)
                    print(f"[Model] ‚úÖ ƒê√£ load weights th√†nh c√¥ng")
                except Exception as load_error:
                    print(f"[Model] ‚ö†Ô∏è Kh√¥ng th·ªÉ load weights (c√≥ th·ªÉ kh√¥ng t∆∞∆°ng th√≠ch): {load_error}")
                    print(f"[Model] S·ª≠ d·ª•ng pretrained model (ch∆∞a fine-tune)")
            else:
                print(f"[Model] ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y {MODEL_PATH}, s·ª≠ d·ª•ng pretrained model (ch∆∞a fine-tune)")
            
            _model.to(DEVICE).eval()
            # T·ªëi ∆∞u memory: set model ·ªü ch·∫ø ƒë·ªô eval v√† disable gradient
            torch.set_grad_enabled(False)
            print(f"[Model] ‚úÖ ƒê√£ load m√¥ h√¨nh th√†nh c√¥ng (device: {DEVICE})")
        except Exception as e:
            print(f"[Model] ‚ùå L·ªói khi load m√¥ h√¨nh: {e}")
            print(f"[Model] S·ª≠ d·ª•ng fallback: rule-based prediction")
            _model_load_failed = True  # ƒê√°nh d·∫•u ƒë√£ fail ƒë·ªÉ kh√¥ng th·ª≠ l·∫°i
            # Fallback: tr·∫£ v·ªÅ prediction ƒë∆°n gi·∫£n d·ª±a tr√™n keywords
            return _fallback_prediction(texts)
    
    # Inference
    results = []
    try:
        for t in texts:
            if not t or pd.isna(t):
                # N·∫øu text r·ªóng, tr·∫£ v·ªÅ prediction m·∫∑c ƒë·ªãnh
                p_m = [0.3] * len(ASPECTS)
                p_s = [0.33, 0.33, 0.34] * len(ASPECTS)
                results.append(p_m + p_s)
                continue
            
            enc = _tokenizer(
                str(t), 
                truncation=True, 
                padding="max_length", 
                max_length=MAX_LEN, 
                return_tensors="pt"
            ).to(DEVICE)
            
            with torch.no_grad():
                logits_m, logits_s = _model(enc["input_ids"], enc["attention_mask"])
                p_m = torch.sigmoid(logits_m)[0].cpu().numpy().tolist()
                p_s = tF.softmax(logits_s, dim=-1)[0].cpu().numpy().flatten().tolist()
                # Gi·∫£i ph√≥ng memory ngay sau khi t√≠nh to√°n
                del enc, logits_m, logits_s
            
            results.append(p_m + p_s)
    except Exception as e:
        print(f"[Model] ‚ö†Ô∏è L·ªói khi inference: {e}, s·ª≠ d·ª•ng fallback")
        return _fallback_prediction(texts)
    
    return pd.Series(results)

def _fallback_prediction(texts: pd.Series) -> pd.Series:
    """Fallback prediction d·ª±a tr√™n keywords n·∫øu model kh√¥ng load ƒë∆∞·ª£c"""
    # Keywords cho t·ª´ng aspect v√† sentiment
    positive_keywords = ["t·ªët", "ƒë·∫πp", "nhanh", "ok", "·ªïn", "h√†i l√≤ng", "tuy·ªát", "xu·∫•t s·∫Øc"]
    negative_keywords = ["x·∫•u", "ch·∫≠m", "t·ªá", "k√©m", "kh√¥ng t·ªët", "th·∫•t v·ªçng", "t·ªìi"]
    
    results = []
    for t in texts:
        if not t or pd.isna(t):
            p_m = [0.3] * len(ASPECTS)
            p_s = [0.33, 0.33, 0.34] * len(ASPECTS)
            results.append(p_m + p_s)
            continue
        
        text_lower = str(t).lower()
        p_m = []
        p_s = []
        
        # ƒê∆°n gi·∫£n: n·∫øu c√≥ t·ª´ kh√≥a t√≠ch c·ª±c/ti√™u c·ª±c th√¨ predict aspect General
        has_positive = any(kw in text_lower for kw in positive_keywords)
        has_negative = any(kw in text_lower for kw in negative_keywords)
        
        for i, asp in enumerate(ASPECTS):
            # Aspect probability (ƒë∆°n gi·∫£n: General c√≥ x√°c su·∫•t cao h∆°n n·∫øu c√≥ keywords)
            if asp == "General" and (has_positive or has_negative):
                p_m.append(0.7)
            else:
                p_m.append(0.2)
            
            # Sentiment probability
            if has_positive and not has_negative:
                p_s.extend([0.6, 0.2, 0.2])  # POS
            elif has_negative and not has_positive:
                p_s.extend([0.2, 0.2, 0.6])  # NEG
            else:
                p_s.extend([0.33, 0.34, 0.33])  # NEU
        
        results.append(p_m + p_s)
    
    return pd.Series(results)

df_pred = df_text.withColumn("predictions", absa_infer_udf(F.col("Review")))

# === 4. Gi·∫£i m√£ k·∫øt qu·∫£ ra nh√£n POS/NEG/NEU ===
@pandas_udf("string")
def decode_sentiment(preds: pd.Series) -> pd.Series:
    SENTIMENTS = ["POS", "NEU", "NEG"]
    res = []
    for p in preds:
        if not p:
            res.append("?")
            continue
        p = list(p)
        p_m, p_s = p[:len(ASPECTS)], p[len(ASPECTS):]
        decoded = []
        for i, asp in enumerate(ASPECTS):
            triplet = p_s[i*3:(i+1)*3]
            s = SENTIMENTS[int(max(range(3), key=lambda j: triplet[j]))]
            decoded.append(f"{asp}:{s}")
        res.append(", ".join(decoded))
    return pd.Series(res)

df_final = df_pred.withColumn("decoded", decode_sentiment(F.col("predictions")))
for asp in ASPECTS:
    df_final = df_final.withColumn(asp, F.regexp_extract("decoded", f"{asp}:(\\w+)", 1))

# === Gi·∫£i m√£ Review JSON ===
review_schema = T.StructType([
    T.StructField("id", T.StringType()),
    T.StructField("review", T.StringType())
])
df_final = df_final.withColumn("ReviewText", from_json(col("Review"), review_schema).getField("review"))

# === 5. Ghi k·∫øt qu·∫£ v√†o PostgreSQL ===
def write_to_postgres(batch_df, batch_id):
    sys.stdout.reconfigure(encoding='utf-8')
    total_rows = batch_df.count()

    if total_rows == 0:
        print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
        return

    preview = batch_df.select("ReviewText", *ASPECTS).limit(5).toPandas().to_dict(orient="records")
    print(f"\n[Batch {batch_id}] Nh·∫≠n {total_rows} d√≤ng, hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu:")
    print(json.dumps(preview, ensure_ascii=False, indent=2))

    try:
        (batch_df
            .select("ReviewText", *ASPECTS)
            .write
            .format("jdbc")
            .option("url", "jdbc:postgresql://postgres:5432/airflow")
            .option("dbtable", "absa_results")
            .option("user", "airflow")
            .option("password", "airflow")
            .option("driver", "org.postgresql.Driver")
            .option("charset", "utf8")
            .mode("append")
            .save()
        )
        print(f"[Batch {batch_id}] ‚úÖ Ghi PostgreSQL th√†nh c√¥ng ({total_rows} d√≤ng).")
    except Exception as e:
        print(f"[Batch {batch_id}] ‚ö†Ô∏è Kh√¥ng th·ªÉ ghi PostgreSQL, ghi log ra console thay th·∫ø.")
        print(f"L·ªói: {str(e)}")
        subset = batch_df.select("ReviewText", *ASPECTS).limit(5).toPandas().to_dict(orient="records")
        print(json.dumps(subset, ensure_ascii=False, indent=2))

# === 6. B·∫Øt ƒë·∫ßu stream ===
query = (
    df_final.writeStream
    .foreachBatch(write_to_postgres)
    .outputMode("append")
    .trigger(processingTime="5 seconds")
    .start()
)

print("üöÄ Streaming job started ‚Äî ƒëang l·∫Øng nghe d·ªØ li·ªáu t·ª´ Kafka...")
query.awaitTermination()
